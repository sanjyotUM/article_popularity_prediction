{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "pd.set_option('max_colwidth', 300)\n",
    "pd.set_option('max_columns', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Mashable Headlines data for training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "mash_data_path = '../Datasets/OnlineNewsPopularity-Mashable/mashable_all_features_with_text.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "hl = pd.read_csv(mash_data_path)[['title', 'shares']].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>shares</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>36367</td>\n",
       "      <td>Why is there a giant pink condom in a Sydney park?</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36368</td>\n",
       "      <td>Gordon Hayward backs up his trash talk by swatting LeBron James</td>\n",
       "      <td>1200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36369</td>\n",
       "      <td>HTC Re camera is bold, but forgets that image is everything</td>\n",
       "      <td>5900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36370</td>\n",
       "      <td>Poppin' bottles: Jay Z buys part of 'Ace of Spades' champagne brand</td>\n",
       "      <td>828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36371</td>\n",
       "      <td>John Lewis unveils new Christmas ad</td>\n",
       "      <td>866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36372</td>\n",
       "      <td>Rare Steinbeck WWII story finally published</td>\n",
       "      <td>942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36373</td>\n",
       "      <td>Los Angeles art museum receives $500 million in rare paintings</td>\n",
       "      <td>952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36374</td>\n",
       "      <td>So, uh, what's up with LeBron James and the languid Cavaliers?</td>\n",
       "      <td>2400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36375</td>\n",
       "      <td>Lorde's dancing does not disappoint in 'Yellow Flicker Beat' video</td>\n",
       "      <td>853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36376</td>\n",
       "      <td>How a TV show from the '80s shaped today's leisure cruise industry</td>\n",
       "      <td>3200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36377</td>\n",
       "      <td>Lyft sues former COO who defected to Uber</td>\n",
       "      <td>1600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36378</td>\n",
       "      <td>NASA wants to build Lytro's camera tech into space probes</td>\n",
       "      <td>11400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36379</td>\n",
       "      <td>Microsoft unveils free new Office apps for iPhone and Android</td>\n",
       "      <td>1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36380</td>\n",
       "      <td>Masked protestors clash with riot police on Bonfire Night in London</td>\n",
       "      <td>3400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36381</td>\n",
       "      <td>Movember has raised more than $500 million since 2004</td>\n",
       "      <td>1300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36382</td>\n",
       "      <td>Is there a net neutrality protest in your city tonight? This map shows you</td>\n",
       "      <td>1300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36383</td>\n",
       "      <td>New 'Into the Woods' trailer has a lot more Meryl, and a lot more singing</td>\n",
       "      <td>690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36384</td>\n",
       "      <td>The high-tech secret hiding in New York City pay phones</td>\n",
       "      <td>4000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36385</td>\n",
       "      <td>Nike drops Adrian Peterson after plea deal</td>\n",
       "      <td>894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36386</td>\n",
       "      <td>Brave little porcupine fends off 17 lions like a boss</td>\n",
       "      <td>3300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                            title  \\\n",
       "36367                          Why is there a giant pink condom in a Sydney park?   \n",
       "36368             Gordon Hayward backs up his trash talk by swatting LeBron James   \n",
       "36369                 HTC Re camera is bold, but forgets that image is everything   \n",
       "36370         Poppin' bottles: Jay Z buys part of 'Ace of Spades' champagne brand   \n",
       "36371                                         John Lewis unveils new Christmas ad   \n",
       "36372                                 Rare Steinbeck WWII story finally published   \n",
       "36373              Los Angeles art museum receives $500 million in rare paintings   \n",
       "36374              So, uh, what's up with LeBron James and the languid Cavaliers?   \n",
       "36375          Lorde's dancing does not disappoint in 'Yellow Flicker Beat' video   \n",
       "36376          How a TV show from the '80s shaped today's leisure cruise industry   \n",
       "36377                                   Lyft sues former COO who defected to Uber   \n",
       "36378                   NASA wants to build Lytro's camera tech into space probes   \n",
       "36379               Microsoft unveils free new Office apps for iPhone and Android   \n",
       "36380         Masked protestors clash with riot police on Bonfire Night in London   \n",
       "36381                       Movember has raised more than $500 million since 2004   \n",
       "36382  Is there a net neutrality protest in your city tonight? This map shows you   \n",
       "36383   New 'Into the Woods' trailer has a lot more Meryl, and a lot more singing   \n",
       "36384                     The high-tech secret hiding in New York City pay phones   \n",
       "36385                                  Nike drops Adrian Peterson after plea deal   \n",
       "36386                       Brave little porcupine fends off 17 lions like a boss   \n",
       "\n",
       "       shares  \n",
       "36367    1000  \n",
       "36368    1200  \n",
       "36369    5900  \n",
       "36370     828  \n",
       "36371     866  \n",
       "36372     942  \n",
       "36373     952  \n",
       "36374    2400  \n",
       "36375     853  \n",
       "36376    3200  \n",
       "36377    1600  \n",
       "36378   11400  \n",
       "36379    1500  \n",
       "36380    3400  \n",
       "36381    1300  \n",
       "36382    1300  \n",
       "36383     690  \n",
       "36384    4000  \n",
       "36385     894  \n",
       "36386    3300  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind = random.randint(0, len(hl)-30)\n",
    "hl.iloc[ind:ind+20, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Feature engineering\n",
    "Syntactic features:\n",
    "1. Total tokens\n",
    "1. Average token length\n",
    "1. Ratio of title cased tokens\n",
    "1. Ratio of upper cased tokens\n",
    "1. Presence of exclamation\n",
    "1. Presence of question mark\n",
    "1. Presence of quote marks\n",
    "\n",
    "\n",
    "\n",
    "Symantic features:\n",
    "1. Three consecutive nouns\n",
    "1. Noun percentage\n",
    "1. Verb percentage\n",
    "1. Proper noun percentage\n",
    "1. Adverb percentage\n",
    "1. Adjective percentage\n",
    "1. Interjection percentage\n",
    "1. Count of non stop-words\n",
    "\n",
    "\n",
    "Smarter features:\n",
    "1. Pantheon, Wikipedia page view features\n",
    "1. Named entity one hot encoding and using best features among all\n",
    "1. Action words (do, stop, fight etc.)\n",
    "1. Urgency words (immediate, now, time etc)\n",
    "1. Hypernym, Hyponym one hot encoding and using best features among all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Syntactic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_syntactic_features(df, hl_col, nlp):\n",
    "    \n",
    "    def total_tokens(doc):\n",
    "        return len(doc)\n",
    "    \n",
    "    def avg_token_len(doc):\n",
    "        return np.mean(np.array([len(x) for x in doc]))\n",
    "    \n",
    "    def title_case_tokens(doc):\n",
    "        return len([x for x in doc if x.shape_[0] == 'X'])/len(doc)\n",
    "    \n",
    "    def upper_case_tokens(doc):\n",
    "        return len([x for x in doc if x.text.isupper()])/len(doc)\n",
    "    \n",
    "    def exclamation_token(doc):\n",
    "        return '!' in doc.text\n",
    "    \n",
    "    def question_mark_token(doc):\n",
    "        return '?' in doc.text\n",
    "    \n",
    "    def quote_mark_token(doc):\n",
    "        return (\"'\" in doc.text) or ('\"' in doc.text)\n",
    "    \n",
    "    def master_loop(doc):\n",
    "        return [\n",
    "            total_tokens(doc),\n",
    "            avg_token_len(doc),\n",
    "            title_case_tokens(doc),\n",
    "            upper_case_tokens(doc),\n",
    "            exclamation_token(doc),\n",
    "            question_mark_token(doc),\n",
    "            quote_mark_token(doc)\n",
    "        ]\n",
    "        \n",
    "    ndf = (\n",
    "        df\n",
    "        .assign(\n",
    "            ans_col = lambda x: x.apply(lambda y: master_loop(nlp(y[hl_col])), axis=1),\n",
    "        )\n",
    "        .assign(\n",
    "            total_tokens = lambda x: (x['ans_col'].str[0]),\n",
    "            avg_token_len = lambda x: (x['ans_col'].str[1]),\n",
    "            title_case_tokens = lambda x: (x['ans_col'].str[2]),\n",
    "            upper_case_tokens = lambda x: (x['ans_col'].str[3]),\n",
    "            exclamation_token = lambda x: x['ans_col'].str[4],\n",
    "            question_mark_token = lambda x: x['ans_col'].str[5],\n",
    "            quote_mark_token = lambda x: x['ans_col'].str[6],\n",
    "        )\n",
    "        .drop(['ans_col'], axis=1)\n",
    "    )\n",
    "    return ndf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_semantic_features(df, hl_col, nlp):\n",
    "    \n",
    "    def get_semantic_features_sentence(doc):\n",
    "        pos_list = [x.pos_ for x in doc]\n",
    "        \n",
    "        def is_noun(pos):\n",
    "                return (pos == 'PROPN') or (pos == 'NOUN')\n",
    "        \n",
    "        def three_consec_nouns(pos_list):\n",
    "            nouns = [is_noun(pos) for pos in pos_list]\n",
    "            for i in range(len(nouns) - 3):\n",
    "                if nouns[i: i+3] == [True, True, True]:\n",
    "                    return True\n",
    "            return False\n",
    "        \n",
    "        def noun_percentage(pos_list):\n",
    "            nouns = [is_noun(pos) for pos in pos_list]\n",
    "            return float(np.mean(np.array(nouns)))\n",
    "        \n",
    "        def proper_noun_percentage(pos_list):\n",
    "            return len([x for x in pos_list if x == 'PROPN'])/len(pos_list)\n",
    "        \n",
    "        def verb_percentage(pos_list):\n",
    "            return len([x for x in pos_list if x == 'VERB'])/len(pos_list)\n",
    "        \n",
    "        def adverb_percentage(pos_list):\n",
    "            return len([x for x in pos_list if x == 'ADV'])/len(pos_list)\n",
    "        \n",
    "        def adjective_percentage(pos_list):\n",
    "            return len([x for x in pos_list if x == 'ADJ'])/len(pos_list)\n",
    "        \n",
    "        def interjection(pos_list):\n",
    "            return 'INTJ' in pos_list\n",
    "        \n",
    "        def non_stop_percentage(doc):\n",
    "            return len([x for x in doc if not x.is_stop])/len(doc)\n",
    "        \n",
    "        return [\n",
    "            three_consec_nouns(pos_list), \n",
    "            noun_percentage(pos_list),\n",
    "            proper_noun_percentage(pos_list),\n",
    "            verb_percentage(pos_list), \n",
    "            adverb_percentage(pos_list),\n",
    "            adjective_percentage(pos_list),\n",
    "            interjection(pos_list),\n",
    "            non_stop_percentage(doc)\n",
    "        ]\n",
    "    \n",
    "    ndf = (\n",
    "        df\n",
    "        .assign(\n",
    "            ans_col = lambda x: x.apply(\n",
    "                lambda y: get_semantic_features_sentence(nlp(y[hl_col])), \n",
    "                axis=1\n",
    "            ),\n",
    "        )\n",
    "        .assign(\n",
    "            three_consec_nouns = lambda x: (x['ans_col'].str[0]),\n",
    "            noun_percentage = lambda x: (x['ans_col'].str[1]),\n",
    "            proper_noun_percentage = lambda x: (x['ans_col'].str[2]),\n",
    "            verb_percentage = lambda x: (x['ans_col'].str[3]),\n",
    "            adverb_percentage = lambda x: x['ans_col'].str[4],\n",
    "            adjective_percentage = lambda x: x['ans_col'].str[5],\n",
    "            interjection = lambda x: x['ans_col'].str[6],\n",
    "            non_stop_percentage = lambda x: x['ans_col'].str[7],\n",
    "        )\n",
    "        .drop(['ans_col'], axis=1)\n",
    "    )\n",
    "    return ndf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating syntactic features...\n",
      "Calculating semantic features...\n",
      "Joining...\n",
      "Done.\n",
      "CPU times: user 9min 30s, sys: 166 ms, total: 9min 30s\n",
      "Wall time: 9min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('Calculating syntactic features...')\n",
    "syntactic_feat = get_syntactic_features(hl, 'title', nlp)\n",
    "print('Calculating semantic features...')\n",
    "semantic_feat = get_semantic_features(hl, 'title', nlp)\n",
    "print('Joining...')\n",
    "all_headline_feat = syntactic_feat.merge(semantic_feat, on=['title', 'shares'])\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_headline_feat.to_csv('../Datasets/OnlineNewsPopularity-Mashable/all_text_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smarter features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load previous features\n",
    "hl_feat = pd.read_csv('../Datasets/OnlineNewsPopularity-Mashable/all_text_features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wikipedia page views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_dataset_path = '../Datasets/Pageviews/pageviews_2008-2013.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanjyot/miniconda3/envs/prnn/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3058: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "include_cols_wiki = ['name', 'countryCode3', 'gender', 'occupation', 'industry', 'domain']\n",
    "year_cols_wiki = ['2012-01', '2012-02', '2012-03', '2012-04', '2012-05', '2012-06', \n",
    "                  '2012-07', '2012-08', '2012-09', '2012-10', '2012-11', '2012-12']\n",
    "\n",
    "wiki = (\n",
    "    pd.read_csv(wiki_dataset_path, sep='\\t')\n",
    "    .loc[lambda x: x['lang'] == 'en']\n",
    "    [include_cols_wiki + year_cols_wiki]\n",
    ")\n",
    "\n",
    "wiki['views'] = 0\n",
    "for col in year_cols_wiki:\n",
    "    wiki['views'] = wiki['views'] + wiki[col]\n",
    "    \n",
    "wiki = wiki.drop(year_cols_wiki, axis=1)\n",
    "wiki['views'] = (wiki['views'] - wiki['views'].min())/(wiki['views'].max() - wiki['views'].min())\n",
    "wiki['name'] = wiki['name'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>countryCode3</th>\n",
       "      <th>gender</th>\n",
       "      <th>occupation</th>\n",
       "      <th>industry</th>\n",
       "      <th>domain</th>\n",
       "      <th>views</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>abraham lincoln</td>\n",
       "      <td>USA</td>\n",
       "      <td>Male</td>\n",
       "      <td>POLITICIAN</td>\n",
       "      <td>GOVERNMENT</td>\n",
       "      <td>INSTITUTIONS</td>\n",
       "      <td>0.687415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>aristotle</td>\n",
       "      <td>GRC</td>\n",
       "      <td>Male</td>\n",
       "      <td>PHILOSOPHER</td>\n",
       "      <td>PHILOSOPHY</td>\n",
       "      <td>HUMANITIES</td>\n",
       "      <td>0.212682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>293</td>\n",
       "      <td>ayn rand</td>\n",
       "      <td>RUS</td>\n",
       "      <td>Female</td>\n",
       "      <td>WRITER</td>\n",
       "      <td>LANGUAGE</td>\n",
       "      <td>HUMANITIES</td>\n",
       "      <td>0.196001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>354</td>\n",
       "      <td>andre agassi</td>\n",
       "      <td>USA</td>\n",
       "      <td>Male</td>\n",
       "      <td>TENNIS PLAYER</td>\n",
       "      <td>INDIVIDUAL SPORTS</td>\n",
       "      <td>SPORTS</td>\n",
       "      <td>0.085859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>419</td>\n",
       "      <td>aldous huxley</td>\n",
       "      <td>GBR</td>\n",
       "      <td>Male</td>\n",
       "      <td>WRITER</td>\n",
       "      <td>LANGUAGE</td>\n",
       "      <td>HUMANITIES</td>\n",
       "      <td>0.079505</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                name countryCode3  gender     occupation            industry  \\\n",
       "30   abraham lincoln          USA    Male     POLITICIAN          GOVERNMENT   \n",
       "160        aristotle          GRC    Male    PHILOSOPHER          PHILOSOPHY   \n",
       "293         ayn rand          RUS  Female         WRITER            LANGUAGE   \n",
       "354     andre agassi          USA    Male  TENNIS PLAYER   INDIVIDUAL SPORTS   \n",
       "419    aldous huxley          GBR    Male         WRITER            LANGUAGE   \n",
       "\n",
       "           domain     views  \n",
       "30   INSTITUTIONS  0.687415  \n",
       "160    HUMANITIES  0.212682  \n",
       "293    HUMANITIES  0.196001  \n",
       "354        SPORTS  0.085859  \n",
       "419    HUMANITIES  0.079505  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_dict = wiki[['name', 'views']].set_index('name')['views'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngram_wiki_score(doc, wiki_dict=wiki_dict):\n",
    "    unigrams = [x.text.lower() for x in doc if (x.is_alpha or x.is_digit) and not (x.is_stop or len(x) < 2)]\n",
    "    bigrams = [' '.join([unigrams[i], unigrams[i+1]]) for i in range(len(unigrams)-1)]\n",
    "    entities = [x.text.lower() for x in doc.ents]\n",
    "    search_grams = unigrams + bigrams + entities\n",
    "\n",
    "    score = 0\n",
    "    for gram in search_grams:\n",
    "        try:\n",
    "            matches = wiki_dict.get(gram)\n",
    "            if matches is None:\n",
    "                gram_score = 0\n",
    "            else:\n",
    "                gram_score = np.array(matches).mean()\n",
    "        except ValueError:\n",
    "            gram_score = 0\n",
    "        \n",
    "        score += gram_score\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 28s, sys: 4.04 s, total: 3min 32s\n",
      "Wall time: 55.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "wiki_score = []\n",
    "\n",
    "for tmpdoc in nlp.pipe(hl_feat['title']):\n",
    "    wiki_score.append(get_ngram_wiki_score(tmpdoc))\n",
    "    \n",
    "hl_feat['wiki_score'] = wiki_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>shares</th>\n",
       "      <th>total_tokens</th>\n",
       "      <th>avg_token_len</th>\n",
       "      <th>title_case_tokens</th>\n",
       "      <th>upper_case_tokens</th>\n",
       "      <th>exclamation_token</th>\n",
       "      <th>question_mark_token</th>\n",
       "      <th>quote_mark_token</th>\n",
       "      <th>three_consec_nouns</th>\n",
       "      <th>noun_percentage</th>\n",
       "      <th>proper_noun_percentage</th>\n",
       "      <th>verb_percentage</th>\n",
       "      <th>adverb_percentage</th>\n",
       "      <th>adjective_percentage</th>\n",
       "      <th>interjection</th>\n",
       "      <th>non_stop_percentage</th>\n",
       "      <th>wiki_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Amazon's Streaming Video Library Now a Little Easier to Navigate</td>\n",
       "      <td>593</td>\n",
       "      <td>11</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>False</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                              title  shares  \\\n",
       "0  Amazon's Streaming Video Library Now a Little Easier to Navigate     593   \n",
       "\n",
       "   total_tokens  avg_token_len  title_case_tokens  upper_case_tokens  \\\n",
       "0            11            5.0           0.727273                0.0   \n",
       "\n",
       "   exclamation_token  question_mark_token  quote_mark_token  \\\n",
       "0              False                False              True   \n",
       "\n",
       "   three_consec_nouns  noun_percentage  proper_noun_percentage  \\\n",
       "0                True         0.363636                0.363636   \n",
       "\n",
       "   verb_percentage  adverb_percentage  adjective_percentage  interjection  \\\n",
       "0         0.090909           0.090909              0.181818         False   \n",
       "\n",
       "   non_stop_percentage  wiki_score  \n",
       "0             0.636364         0.0  "
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hl_feat.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "X_train_path = '../Datasets/OnlineNewsPopularity-Mashable/all_text_feat_X_train.csv'\n",
    "y_train_path = '../Datasets/OnlineNewsPopularity-Mashable/all_text_feat_y_train.csv'\n",
    "X_test_path = '../Datasets/OnlineNewsPopularity-Mashable/all_text_feat_X_test.csv'\n",
    "y_test_path = '../Datasets/OnlineNewsPopularity-Mashable/all_text_feat_y_test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_headline_feat['popular'] = all_headline_feat['shares'] > 1400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "syntactic_features = ['total_tokens', 'avg_token_len', 'title_case_tokens', \n",
    "                      'upper_case_tokens', 'exclamation_token', 'question_mark_token', \n",
    "                      'quote_mark_token']\n",
    "semantic_features = ['three_consec_nouns', 'noun_percentage', 'proper_noun_percentage', \n",
    "                     'verb_percentage' ,'adverb_percentage', 'adjective_percentage', \n",
    "                     'interjection', 'non_stop_percentage']\n",
    "\n",
    "final_features = syntactic_features + semantic_features\n",
    "label = 'popular'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = all_headline_feat[final_features]\n",
    "y = all_headline_feat[label]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "X_train.to_csv(X_train_path, index=False)\n",
    "X_test.to_csv(X_test_path, index=False)\n",
    "y_train.to_csv(y_train_path, index=False)\n",
    "y_test.to_csv(y_test_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "hl_X_train, hl_X_test, hl_y_train, hl_y_test = train_test_split(X_train, y_train, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(\n",
    "    n_estimators=400, \n",
    "    max_depth=5, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanjyot/miniconda3/envs/prnn/lib/python3.7/site-packages/ipykernel_launcher.py:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.58 s, sys: 7.96 ms, total: 3.58 s\n",
      "Wall time: 3.58 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=5, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=400,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Train\n",
    "clf.fit(hl_X_train, hl_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title_case_tokens         0.184983\n",
       "noun_percentage           0.125702\n",
       "avg_token_len             0.121856\n",
       "proper_noun_percentage    0.116707\n",
       "non_stop_percentage       0.085985\n",
       "total_tokens              0.072720\n",
       "upper_case_tokens         0.069588\n",
       "verb_percentage           0.066584\n",
       "adjective_percentage      0.058950\n",
       "adverb_percentage         0.050194\n",
       "three_consec_nouns        0.017954\n",
       "quote_mark_token          0.011998\n",
       "question_mark_token       0.006688\n",
       "exclamation_token         0.005226\n",
       "interjection              0.004865\n",
       "dtype: float64"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(data=clf.feature_importances_, index=final_features).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5416876870816938 0.5240112994350282\n"
     ]
    }
   ],
   "source": [
    "pred_train = clf.predict(X_train)\n",
    "pred_test = clf.predict(X_test)\n",
    "\n",
    "acc_train = accuracy_score(pred_train, y_train)\n",
    "acc_test = accuracy_score(pred_test, y_test)\n",
    "print(acc_train, acc_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
